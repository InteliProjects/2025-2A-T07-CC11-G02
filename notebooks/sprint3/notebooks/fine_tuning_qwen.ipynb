{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29fZnzJ7Huq"
      },
      "source": [
        "\n",
        "# _Fine-tuning_ de modelo Qwen\n",
        "Este notebook realiza o _fine-tuning_ do modelo **Qwen/Qwen2.5-0.5B** utilizando o m√©todo **LoRA** (Low-Rank Adaptation), focado no dom√≠nio de moda.  \n",
        "O objetivo √© treinar o modelo para responder perguntas espec√≠ficas sobre moda com base em um conjunto supervisionado de **perguntas e respostas** contidas em um arquivo CSV.\n",
        "\n",
        "O CSV foi gerado em duas etapas:\n",
        "1. Transcri√ß√£o dos v√≠deos da Curadobia\n",
        "    - Utilizamos modelos de Speech-to-text para gerar transcri√ß√µes de diversos v√≠deos sobre moda produzidos pelos parceiros de projeto.\n",
        "2. Uso de LLM para estruturar os dados\n",
        "    - A partir de um `.txt` gerado pela transcri√ß√£o, utilizamos um LLM para gerar perguntas e respostas baseadas no `.txt`. O processo foi feito em etapas com supervis√£o humana.\n",
        "\n",
        "## Estrutura do processo de _fine-tuning_\n",
        "1. **Prepara√ß√£o do ambiente**  \n",
        "   - Instala√ß√£o de depend√™ncias necess√°rias.\n",
        "   - Montagem do Google Drive para leitura dos dados e salvamento do modelo final.\n",
        "\n",
        "2. **Processamento de dados**  \n",
        "   - Leitura do CSV que cont√©m colunas `input` (pergunta) e `output` (resposta).\n",
        "   - Limpeza e formata√ß√£o dos dados no formato de chat para treino do modelo.\n",
        "\n",
        "3. **Treinamento com LoRA**  \n",
        "   - Aplica√ß√£o do m√©todo LoRA para treinar apenas partes espec√≠ficas do modelo (proje√ß√µes de aten√ß√£o).\n",
        "   - Treinamento silencioso para evitar polui√ß√£o visual no notebook.\n",
        "\n",
        "4. **Avalia√ß√£o e Salvamento**  \n",
        "   - Avalia√ß√£o final do modelo.\n",
        "   - Mesclagem do backbone com os adapters LoRA em um √∫nico modelo e salvamento em `.pkl` e `.safetensors`.\n",
        "\n",
        "5. **Infer√™ncia e Compara√ß√£o**  \n",
        "   - Defini√ß√£o de fun√ß√µes para comparar o comportamento do modelo **base** e do modelo **fine-tunado**.\n",
        "   - Execu√ß√£o de perguntas manuais para observar a melhoria obtida ap√≥s o fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzt9rSd43d_"
      },
      "source": [
        "# Configura√ß√µes iniciais\n",
        "\n",
        "Esta c√©lula prepara o ambiente para o treinamento:\n",
        "\n",
        "- Define seeds para reprodutibilidade dos resultados.\n",
        "- Configura caminhos absolutos para os arquivos no Google Drive:\n",
        "  - **CSV** com perguntas e respostas.\n",
        "  - **Modelo final** salvo em `.pkl`.\n",
        "- Monta o Google Drive (Colab) para acesso aos dados.\n",
        "- Seleciona automaticamente GPU, se dispon√≠vel.\n",
        "- Carrega o tokenizer do Qwen e ajusta o token de padding.\n",
        "- Define o texto de sistema (`SYSTEM_PREFIX`), que estabelece o contexto do modelo como consultora de moda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n15iSC158Iif"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel 'Python 3.12.8' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "!pip install -U -q transformers peft accelerate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx4KtYWM01zE"
      },
      "outputs": [],
      "source": [
        "# ===== Imports & Config =====\n",
        "import os, re, json, random, pickle, sys, platform\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "# Seeds / Modelo base\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "# Caminhos absolutos\n",
        "DATA_CSV    = \"/content/drive/Shareddrives/nsync_m11/sprint3/perguntas_respostas_1000.csv\"\n",
        "PICKLE_PATH = \"/content/drive/Shareddrives/nsync_m11/sprint3/qwen_fine_tuned.pkl\"\n",
        "SAFETENSORS_PATH  = \"/content/drive/Shareddrives/nsync_m11/sprint3/qwen_fine_tuned.safetensors\"\n",
        "\n",
        "# Monta Google Drive (modelo foi desenvolvido usando o Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "# Tom do sistema\n",
        "SYSTEM_PREFIX = \"Voc√™ √© uma consultora de moda brasileira. Responda de forma clara, objetiva e elegante.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRft72oy5EVZ"
      },
      "source": [
        "# Leitura e limpeza do CSV\n",
        "\n",
        "- L√™ o CSV que cont√©m as colunas:\n",
        "  - **input**: pergunta.\n",
        "  - **output**: resposta.\n",
        "- Valida a presen√ßa dessas colunas.\n",
        "- Remove linhas vazias ou com dados inv√°lidos.\n",
        "- Renomeia a coluna `input` para `instruction` para padroniza√ß√£o interna do pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GGAKBEkrbVA"
      },
      "outputs": [],
      "source": [
        "csv_path = Path(DATA_CSV)\n",
        "assert csv_path.exists(), f\"CSV n√£o encontrado: {DATA_CSV}\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "assert \"input\" in df.columns and \"output\" in df.columns, f\"CSV deve conter colunas 'input' e 'output'. Colunas: {list(df.columns)}\"\n",
        "\n",
        "df = df.rename(columns={\"input\": \"instruction\", \"output\": \"output\"})\n",
        "df[\"instruction\"] = df[\"instruction\"].astype(str).str.strip()\n",
        "df[\"output\"] = df[\"output\"].astype(str).str.strip()\n",
        "df = df[(df[\"instruction\"] != \"\") & (df[\"output\"] != \"\")].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-ftBeUG5K_X"
      },
      "source": [
        "# Convers√£o para formato de chat\n",
        "\n",
        "Transforma cada exemplo do CSV em um formato de **conversa estruturada**, utilizado no treino e na infer√™ncia.  \n",
        "O formato final ser√°:\n",
        "\n",
        "```<|system|>\n",
        "{SYSTEM_PREFIX}\n",
        "<|user|>\n",
        "{instruction}\n",
        "<|assistant|>\n",
        "{output}\n",
        "```\n",
        "\n",
        "Esse formato garante consist√™ncia no processo, facilitando que o modelo aprenda a diferenciar perguntas de respostas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTi_1u6fsRul"
      },
      "outputs": [],
      "source": [
        "# Opcional: limite para testes r√°pidos (None = usa tudo)\n",
        "MAX_EXAMPLES = None\n",
        "df_trainable = df.iloc[:MAX_EXAMPLES] if MAX_EXAMPLES else df\n",
        "\n",
        "def to_text_row(instr: str, output: str, input_text: str = \"\") -> str:\n",
        "    if input_text:\n",
        "        return (\n",
        "            f\"<|system|>\\n{SYSTEM_PREFIX}\\n\"\n",
        "            f\"<|user|>\\n{instr}\\n\\nContexto:\\n{input_text}\\n\"\n",
        "            f\"<|assistant|>\\n{output}\\n\"\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            f\"<|system|>\\n{SYSTEM_PREFIX}\\n\"\n",
        "            f\"<|user|>\\n{instr}\\n\"\n",
        "            f\"<|assistant|>\\n{output}\\n\"\n",
        "        )\n",
        "\n",
        "texts = [to_text_row(r[\"instruction\"], r[\"output\"]) for _, r in df_trainable.iterrows()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGZ4Rx3b5PlV"
      },
      "source": [
        "# Cria√ß√£o do dataset e tokeniza√ß√£o\n",
        "\n",
        "1. Cria um objeto `Dataset` com todos os exemplos.\n",
        "2. Divide em **treino (90%)** e **valida√ß√£o (10%)** para avalia√ß√£o durante e ap√≥s o treinamento.\n",
        "3. Tokeniza os textos:\n",
        "   - Limite m√°ximo de 1024 tokens por amostra.\n",
        "   - Truncamento autom√°tico de textos muito longos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwn1aaJvsSwE"
      },
      "outputs": [],
      "source": [
        "ds = Dataset.from_dict({\"text\": texts})\n",
        "ds = ds.train_test_split(test_size=0.1, seed=SEED)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, max_length=1024)\n",
        "\n",
        "ds_tok = ds.map(tokenize, batched=True, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqurCc-v5dvl"
      },
      "source": [
        "# Carregamento do modelo e aplica√ß√£o do LoRA\n",
        "\n",
        "- Carrega o modelo base **Qwen/Qwen2.5-0.5B**.\n",
        "- Aplica **LoRA** (Low-Rank Adaptation), que permite treinar apenas partes espec√≠ficas do modelo:\n",
        "  - Camadas de proje√ß√£o da aten√ß√£o (`q_proj`, `k_proj`, `v_proj`, `o_proj`).\n",
        "- Essa abordagem reduz o custo computacional e mant√©m a maior parte dos par√¢metros congelados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLf0_REEsVJD"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWu0xDXM5jJ9"
      },
      "source": [
        "# Configura√ß√£o do treinamento\n",
        "\n",
        "- Define os hiperpar√¢metros principais:\n",
        "  - `batch_size`, `epochs`, `learning_rate`.\n",
        "  - Estrat√©gias de logging e salvamento desativadas para manter o notebook limpo.\n",
        "- Utiliza `DataCollatorForLanguageModeling` configurado para **modelos causais** (`mlm=False`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErsZz6AesXI3"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./_tmp_session_only\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    logging_strategy=\"no\",\n",
        "    bf16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_tok[\"train\"],\n",
        "    eval_dataset=ds_tok[\"test\"],\n",
        "    data_collator=collator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiIe0bKH5uK2"
      },
      "source": [
        "# Treinamento do modelo\n",
        "\n",
        "Inicia o processo de treinamento com LoRA aplicado ao backbone Qwen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_pBQS2YstfP"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ing_CUFc5z9T"
      },
      "source": [
        "# Merge do modelo e salvamento\n",
        "\n",
        "- Ap√≥s o treino, os pesos do LoRA s√£o **mesclados** ao modelo base.  \n",
        "- O resultado final √© salvo em **dois formatos**:\n",
        "  - `.pkl` ‚Äì compat√≠vel com carregamento via `pickle`, √∫til para scripts Python simples.  \n",
        "  - `.safetensors` ‚Äì formato mais seguro e eficiente, recomendado para uso em produ√ß√£o ou compartilhamento.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIe6cBRIszII"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    merged = model.merge_and_unload()\n",
        "except AttributeError:\n",
        "    merged = model.base_model.merge_and_unload()\n",
        "\n",
        "# Cria diret√≥rios se n√£o existirem\n",
        "out_dir = Path(PICKLE_PATH).parent\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Salvar em .pkl ---\n",
        "with open(PICKLE_PATH, \"wb\") as f:\n",
        "    pickle.dump(merged.state_dict(), f)\n",
        "\n",
        "# --- Salvar em .safetensors ---\n",
        "merged.save_pretrained(out_dir, safe_serialization=True)\n",
        "tok.save_pretrained(out_dir)\n",
        "\n",
        "print(f\"Modelo salvo em:\\n- {PICKLE_PATH}\\n- {out_dir}/model.safetensors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QAd5cD550z"
      },
      "source": [
        "# Fun√ß√µes de infer√™ncia\n",
        "\n",
        "Define fun√ß√µes auxiliares para:\n",
        "- Carregar o modelo base para compara√ß√£o.\n",
        "- Montar prompts de entrada no formato esperado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clIA7aRXs1F0"
      },
      "outputs": [],
      "source": [
        "# Modelo base para compara√ß√£o\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.eval()\n",
        "merged.eval()\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    return f\"<|system|>\\n{SYSTEM_PREFIX}\\n<|user|>\\n{user_msg}\\n<|assistant|>\\n\"\n",
        "\n",
        "def generate_only_new(model, prompt: str, max_new_tokens=200, temperature=0.7, top_p=0.9, seed=123):\n",
        "    torch.manual_seed(seed)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        eos_token_id=tok.eos_token_id\n",
        "    )\n",
        "    new_tokens = out[0][input_len:]\n",
        "    text = tok.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "    if \"<|user|>\" in text:\n",
        "        text = text.split(\"<|user|>\")[0].strip()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHd1sGCm6AsS"
      },
      "source": [
        "# Compara√ß√£o entre modelo base e fine-tunado\n",
        "\n",
        "Permite comparar a performance do modelo base e do modelo ajustado em **3 perguntas escolhidas manualmente**:\n",
        "\n",
        "1. `Q1`, `Q2` e `Q3` s√£o perguntas presentes no CSV.\n",
        "2. A c√©lula exibir√° lado a lado:\n",
        "   - Resposta do Qwen base.\n",
        "   - Resposta do Qwen ap√≥s fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV_1fgdws2HE"
      },
      "outputs": [],
      "source": [
        "Q1 = \"Quais marcas oferecem boas regatas b√°sicas?\"\n",
        "Q2 = \"Como combinar t√™nis em looks elegantes?\"\n",
        "Q3 = \"Que cor de bolsa combina com roupas escuras al√©m do preto?\"\n",
        "\n",
        "tests = [q for q in [Q1, Q2, Q3] if isinstance(q, str) and q.strip()]\n",
        "\n",
        "# Executa compara√ß√£o para cada pergunta preenchida\n",
        "results = []\n",
        "for idx, q in enumerate(tests, 1):\n",
        "    prompt = build_prompt(q)\n",
        "    base_ans = generate_only_new(base_model, prompt, max_new_tokens=220, temperature=0.1, top_p=0.9, seed=idx*11)\n",
        "    ft_ans   = generate_only_new(merged,     prompt, max_new_tokens=220, temperature=0.1, top_p=0.9, seed=idx*11)\n",
        "    results.append((q, base_ans, ft_ans))\n",
        "\n",
        "# Exibe resultados de forma simples\n",
        "for i, (q, base_ans, ft_ans) in enumerate(results, 1):\n",
        "    print(\"=\"*80)\n",
        "    print(f\"[Pergunta {i}]\")\n",
        "    print(q)\n",
        "    print(\"-\"*80)\n",
        "    print(\"[QWEN BASE]\")\n",
        "    print(base_ans)\n",
        "    print(\"-\"*80)\n",
        "    print(\"[QWEN FINE-TUNING]\")\n",
        "    print(ft_ans)\n",
        "print(\"=\"*80 if results else \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJJtdTutsF-6"
      },
      "source": [
        "# An√°lise Sem√¢ntica Avan√ßada de Clusters\n",
        "\n",
        "Esta se√ß√£o implementa uma an√°lise sem√¢ntica avan√ßada para visualizar e quantificar clusters de perguntas similares.\n",
        "O objetivo √© entender como o modelo organiza semanticamente as perguntas e identificar padr√µes de classifica√ß√£o.\n",
        "\n",
        "## Funcionalidades:\n",
        "1. **Extra√ß√£o de Embeddings**: Extrai representa√ß√µes sem√¢nticas das perguntas usando o modelo fine-tunado\n",
        "2. **M√∫ltiplas T√©cnicas de Redu√ß√£o de Dimens√£o**: t-SNE, UMAP e PCA\n",
        "3. **Algoritmos de Clustering**: K-means, DBSCAN e Clustering Hier√°rquico\n",
        "4. **Visualiza√ß√µes Interativas**: Gr√°ficos 2D e 3D com Plotly\n",
        "5. **M√©tricas de Avalia√ß√£o**: Silhouette Score, Calinski-Harabasz Index\n",
        "6. **An√°lise de Varia√ß√µes Lingu√≠sticas**: Identifica√ß√£o de perguntas similares com diferentes formula√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-8eD6aHsGvz"
      },
      "outputs": [],
      "source": [
        "CLUSTERING_CONFIG = {\n",
        "    \"max_samples\": 2000,\n",
        "    \"embedding_strategy\": \"last_hidden_state\",\n",
        "    \"tsne_perplexity\": 30,\n",
        "    \"tsne_learning_rate\": 200,\n",
        "    \"umap_n_neighbors\": 15,\n",
        "    \"umap_min_dist\": 0.1,\n",
        "    \"n_clusters_range\": range(2, 21),\n",
        "    \"random_state\": SEED\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YiLPTx5sJVd"
      },
      "outputs": [],
      "source": [
        "# Extrai representa√ß√µes sem√¢nticas das perguntas usando o modelo fine-tunado\n",
        "# Converte texto em vetores num√©ricos que capturam o significado sem√¢ntico\n",
        "def extract_question_embeddings(model, tokenizer, questions, strategy=\"last_hidden_state\", max_length=512):\n",
        "\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for question in questions:\n",
        "            # Tokeniza a pergunta\n",
        "            inputs = tokenizer(\n",
        "                question,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                padding=True\n",
        "            ).to(next(model.parameters()).device)\n",
        "\n",
        "            # Obt√©m outputs do modelo\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            if strategy == \"last_hidden_state\":\n",
        "                # Usa o √∫ltimo token (CLS-like)\n",
        "                embedding = outputs.hidden_states[-1][:, -1, :].float().cpu().numpy()\n",
        "            elif strategy == \"mean\":\n",
        "                # M√©dia dos tokens (ignorando padding)\n",
        "                attention_mask = inputs[\"attention_mask\"]\n",
        "                hidden_states = outputs.hidden_states[-1].float()\n",
        "                mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "                masked_embeddings = hidden_states * mask\n",
        "                sum_embeddings = torch.sum(masked_embeddings, dim=1)\n",
        "                sum_mask = torch.sum(mask, dim=1)\n",
        "                embedding = (sum_embeddings / sum_mask).cpu().numpy()\n",
        "            elif strategy == \"pooler\":\n",
        "                # Usa pooler se dispon√≠vel, sen√£o usa last_hidden_state\n",
        "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                    embedding = outputs.pooler_output.float().cpu().numpy()\n",
        "                else:\n",
        "                    embedding = outputs.hidden_states[-1][:, -1, :].float().cpu().numpy()\n",
        "            else:\n",
        "                raise ValueError(f\"Estrat√©gia '{strategy}' n√£o suportada\")\n",
        "\n",
        "            embeddings.append(embedding[0])\n",
        "\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9PqFM9OsMIN"
      },
      "outputs": [],
      "source": [
        "# Reduz a dimensionalidade dos embeddings para visualiza√ß√£o em 2D\n",
        "# Aplica PCA, t-SNE e UMAP para criar representa√ß√µes visuais dos dados\n",
        "def reduce_dimensions(embeddings, config):\n",
        "\n",
        "    # Normaliza√ß√£o\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "\n",
        "    reductions = {}\n",
        "\n",
        "    # PCA\n",
        "    print(\"Aplicando PCA...\")\n",
        "    pca = PCA(n_components=50, random_state=config[\"random_state\"])\n",
        "    reductions[\"pca_50d\"] = pca.fit_transform(embeddings_scaled)\n",
        "\n",
        "    pca_2d = PCA(n_components=2, random_state=config[\"random_state\"])\n",
        "    reductions[\"pca_2d\"] = pca_2d.fit_transform(embeddings_scaled)\n",
        "\n",
        "    # t-SNE\n",
        "    print(\"Aplicando t-SNE...\")\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        perplexity=config[\"tsne_perplexity\"],\n",
        "        learning_rate=config[\"tsne_learning_rate\"],\n",
        "        random_state=config[\"random_state\"],\n",
        "        max_iter=1000\n",
        "    )\n",
        "    reductions[\"tsne_2d\"] = tsne.fit_transform(embeddings_scaled)\n",
        "\n",
        "    # UMAP\n",
        "    print(\"Aplicando UMAP...\")\n",
        "    umap_reducer = umap.UMAP(\n",
        "        n_components=2,\n",
        "        n_neighbors=config[\"umap_n_neighbors\"],\n",
        "        min_dist=config[\"umap_min_dist\"],\n",
        "        random_state=config[\"random_state\"]\n",
        "    )\n",
        "    reductions[\"umap_2d\"] = umap_reducer.fit_transform(embeddings_scaled)\n",
        "\n",
        "    return reductions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPjybwXksNlH"
      },
      "outputs": [],
      "source": [
        "# Encontra o n√∫mero ideal de clusters usando m√©tricas de qualidade\n",
        "# Testa diferentes n√∫meros de clusters e escolhe o melhor baseado no Silhouette Score\n",
        "def find_optimal_clusters(embeddings, config):\n",
        "\n",
        "    # Normaliza√ß√£o\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "\n",
        "    silhouette_scores = []\n",
        "    calinski_scores = []\n",
        "    inertias = []\n",
        "\n",
        "    for n_clusters in config[\"n_clusters_range\"]:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=config[\"random_state\"], n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(embeddings_scaled)\n",
        "\n",
        "        # Silhouette Score\n",
        "        sil_score = silhouette_score(embeddings_scaled, cluster_labels)\n",
        "        silhouette_scores.append(sil_score)\n",
        "\n",
        "        # Calinski-Harabasz Index\n",
        "        cal_score = calinski_harabasz_score(embeddings_scaled, cluster_labels)\n",
        "        calinski_scores.append(cal_score)\n",
        "\n",
        "        # Inertia\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # Encontra o n√∫mero √≥timo\n",
        "    optimal_sil = config[\"n_clusters_range\"][np.argmax(silhouette_scores)]\n",
        "    optimal_cal = config[\"n_clusters_range\"][np.argmax(calinski_scores)]\n",
        "\n",
        "    return {\n",
        "        \"n_clusters_range\": list(config[\"n_clusters_range\"]),\n",
        "        \"silhouette_scores\": silhouette_scores,\n",
        "        \"calinski_scores\": calinski_scores,\n",
        "        \"inertias\": inertias,\n",
        "        \"optimal_silhouette\": optimal_sil,\n",
        "        \"optimal_calinski\": optimal_cal\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-hPHUCisPOc"
      },
      "outputs": [],
      "source": [
        "# Aplica algoritmos de clustering para agrupar perguntas similares\n",
        "# Usa K-Means, DBSCAN e Clustering Hier√°rquico para validar os resultados\n",
        "def apply_clustering_algorithms(embeddings, n_clusters, config):\n",
        "\n",
        "    # Normaliza√ß√£o\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # K-Means\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=config[\"random_state\"], n_init=10)\n",
        "    results[\"kmeans\"] = kmeans.fit_predict(embeddings_scaled)\n",
        "\n",
        "    # DBSCAN\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    results[\"dbscan\"] = dbscan.fit_predict(embeddings_scaled)\n",
        "\n",
        "    # Clustering Hier√°rquico\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    results[\"hierarchical\"] = hierarchical.fit_predict(embeddings_scaled)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFSHzTNLsQrD"
      },
      "outputs": [],
      "source": [
        "# Cria as 3 visualiza√ß√µes principais da an√°lise sem√¢ntica\n",
        "# Gera gr√°ficos interativos mostrando clusters, densidade e similaridade\n",
        "def create_main_visualizations(questions, embeddings, reductions, cluster_results, optimal_n_clusters):\n",
        "\n",
        "    print(\"Criando visualiza√ß√µes principais...\")\n",
        "\n",
        "    print(\"Gerando Gr√°fico 1: Clusters 2D com t-SNE...\")\n",
        "    fig1 = go.Figure()\n",
        "    colors = px.colors.qualitative.Set3 + px.colors.qualitative.Pastel1 + px.colors.qualitative.Pastel2\n",
        "\n",
        "    # Adicionar pontos com tamanho baseado na densidade do cluster\n",
        "    for cluster_id in range(optimal_n_clusters):\n",
        "        mask = cluster_results[\"kmeans\"] == cluster_id\n",
        "        if np.any(mask):\n",
        "            cluster_questions = [questions[i] for i in range(len(questions)) if mask[i]]\n",
        "            cluster_size = len(cluster_questions)\n",
        "\n",
        "            # Tamanho dos pontos baseado no tamanho do cluster\n",
        "            point_size = max(8, min(20, 8 + (cluster_size / 50)))\n",
        "\n",
        "            fig1.add_trace(\n",
        "        go.Scatter(\n",
        "                    x=reductions[\"tsne_2d\"][mask, 0],\n",
        "                    y=reductions[\"tsne_2d\"][mask, 1],\n",
        "            mode='markers',\n",
        "                    marker=dict(\n",
        "                        size=point_size,\n",
        "                        color=colors[cluster_id % len(colors)],\n",
        "                        opacity=0.7,\n",
        "                        line=dict(width=1.5, color='white'),\n",
        "                        symbol='circle'\n",
        "                    ),\n",
        "                    text=[f\"Cluster {cluster_id}: {q[:50]}...\" for q in cluster_questions],\n",
        "                    hovertemplate='<b>%{text}</b><br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>',\n",
        "                    name=f'Cluster {cluster_id} ({cluster_size} perguntas)',\n",
        "                    legendgroup=f'cluster_{cluster_id}',\n",
        "                    showlegend=True\n",
        "                )\n",
        "            )\n",
        "\n",
        "    fig1.update_layout(\n",
        "        title=dict(\n",
        "            text=\"Gr√°fico 1: Visualiza√ß√£o dos Clusters Sem√¢nticos (t-SNE 2D)\",\n",
        "            font=dict(size=18, family=\"Arial Black\")\n",
        "        ),\n",
        "        xaxis_title=\"t-SNE Dimens√£o 1\",\n",
        "        yaxis_title=\"t-SNE Dimens√£o 2\",\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        showlegend=True,\n",
        "        template=\"plotly_white\",\n",
        "        margin=dict(l=80, r=80, t=100, b=80),\n",
        "        legend=dict(\n",
        "            x=1.02,\n",
        "            y=1,\n",
        "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "            bordercolor=\"Black\",\n",
        "            borderwidth=1,\n",
        "            font=dict(size=12)\n",
        "        ),\n",
        "        xaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig1.show()\n",
        "\n",
        "    print(\"Gerando Gr√°fico 2: An√°lise de Densidade...\")\n",
        "    cluster_stats = []\n",
        "    for cluster_id in range(optimal_n_clusters):\n",
        "        mask = cluster_results[\"kmeans\"] == cluster_id\n",
        "        if np.any(mask):\n",
        "            cluster_embeddings = embeddings[mask]\n",
        "\n",
        "            from sklearn.metrics.pairwise import euclidean_distances\n",
        "            distances = euclidean_distances(cluster_embeddings)\n",
        "            np.fill_diagonal(distances, np.inf)\n",
        "            avg_distance = np.mean(distances[distances != np.inf])\n",
        "            from sklearn.metrics.pairwise import cosine_similarity\n",
        "            similarities = cosine_similarity(cluster_embeddings)\n",
        "            np.fill_diagonal(similarities, 0)\n",
        "            avg_similarity = np.mean(similarities[similarities != 0])\n",
        "\n",
        "            cluster_stats.append({\n",
        "                'cluster_id': cluster_id,\n",
        "                'size': np.sum(mask),\n",
        "                'avg_distance': avg_distance,\n",
        "                'avg_similarity': avg_similarity,\n",
        "                'density_score': 1 / (1 + avg_distance)\n",
        "            })\n",
        "\n",
        "    cluster_stats = pd.DataFrame(cluster_stats)\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    # Criar cores √∫nicas para cada cluster\n",
        "    colors = px.colors.qualitative.Set3 + px.colors.qualitative.Pastel1 + px.colors.qualitative.Pastel2\n",
        "\n",
        "    # Primeiro trace: c√≠rculos sem texto, apenas com cores √∫nicas\n",
        "    fig2.add_trace(\n",
        "        go.Scatter(\n",
        "            x=cluster_stats['avg_distance'],\n",
        "            y=cluster_stats['avg_similarity'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=cluster_stats['size'] * 2.5,\n",
        "                color=[colors[int(cid) % len(colors)] for cid in cluster_stats['cluster_id']],\n",
        "                opacity=0.8,\n",
        "                line=dict(width=3, color='white'),\n",
        "                sizemode='diameter',\n",
        "                sizemin=20\n",
        "            ),\n",
        "            hovertemplate='<b>Cluster %{customdata}</b><br>Size: %{marker.size}<br>Avg Distance: %{x:.3f}<br>Avg Similarity: %{y:.3f}<extra></extra>',\n",
        "            customdata=cluster_stats['cluster_id'],\n",
        "            name='Clusters',\n",
        "            showlegend=False\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Adicionar uma tabela de legenda no canto superior direito\n",
        "    legend_x = 0.98\n",
        "    legend_y = 0.95\n",
        "\n",
        "    # T√≠tulo da legenda\n",
        "    fig2.add_annotation(\n",
        "        x=legend_x - 0.04,\n",
        "        y=legend_y + 0.03,\n",
        "        text=\"<b>Legenda dos Clusters</b>\",\n",
        "        showarrow=False,\n",
        "        font=dict(size=14, color='black'),\n",
        "        xanchor=\"center\",\n",
        "        yanchor=\"middle\"\n",
        "    )\n",
        "\n",
        "    for i, (cluster_id, size, avg_dist, avg_sim) in enumerate(zip(\n",
        "        cluster_stats['cluster_id'],\n",
        "        cluster_stats['size'],\n",
        "        cluster_stats['avg_distance'],\n",
        "        cluster_stats['avg_similarity']\n",
        "    )):\n",
        "        y_offset = 0.02 * (len(cluster_stats) - i - 1)\n",
        "\n",
        "        # Quadrado colorido da legenda\n",
        "        fig2.add_shape(\n",
        "            type=\"rect\",\n",
        "            x0=legend_x - 0.08, y0=legend_y - y_offset - 0.008,\n",
        "            x1=legend_x - 0.06, y1=legend_y - y_offset + 0.008,\n",
        "            fillcolor=colors[int(cluster_id) % len(colors)],\n",
        "            line=dict(width=1, color='white'),\n",
        "            layer=\"above\"\n",
        "        )\n",
        "\n",
        "        # Texto da legenda\n",
        "        fig2.add_annotation(\n",
        "            x=legend_x - 0.05,\n",
        "            y=legend_y - y_offset,\n",
        "            text=f\"Cluster {int(cluster_id)}: {int(size)} perguntas\",\n",
        "            showarrow=False,\n",
        "            font=dict(size=12, color='black'),\n",
        "            xanchor=\"left\",\n",
        "            yanchor=\"middle\"\n",
        "        )\n",
        "\n",
        "    fig2.update_layout(\n",
        "        title=dict(\n",
        "            text=\"Gr√°fico 2: An√°lise de Densidade e Coes√£o dos Clusters\",\n",
        "            font=dict(size=18, family=\"Arial Black\")\n",
        "        ),\n",
        "        xaxis_title=\"Dist√¢ncia M√©dia Interna (menor = mais denso)\",\n",
        "        yaxis_title=\"Similaridade M√©dia Interna (maior = mais coeso)\",\n",
        "        width=1200,\n",
        "        height=700,\n",
        "        template=\"plotly_white\",\n",
        "        margin=dict(l=80, r=200, t=100, b=80),\n",
        "        xaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig2.show()\n",
        "\n",
        "    print(\"Gerando Gr√°fico 3: Heatmap de Similaridade...\")\n",
        "    cluster_centroids = []\n",
        "    for cluster_id in range(optimal_n_clusters):\n",
        "        mask = cluster_results[\"kmeans\"] == cluster_id\n",
        "        if np.any(mask):\n",
        "            centroid = np.mean(embeddings[mask], axis=0)\n",
        "            cluster_centroids.append(centroid)\n",
        "        else:\n",
        "            cluster_centroids.append(np.zeros(embeddings.shape[1]))\n",
        "\n",
        "    cluster_centroids = np.array(cluster_centroids)\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity_matrix = cosine_similarity(cluster_centroids)\n",
        "\n",
        "    # Criar matriz de texto com melhor formata√ß√£o\n",
        "    text_matrix = []\n",
        "    for i in range(optimal_n_clusters):\n",
        "        row = []\n",
        "        for j in range(optimal_n_clusters):\n",
        "            if i == j:\n",
        "                row.append(\"1.000\")  # Diagonal sempre 1.000\n",
        "            else:\n",
        "                row.append(f\"{similarity_matrix[i][j]:.3f}\")\n",
        "        text_matrix.append(row)\n",
        "\n",
        "    fig3 = go.Figure(data=go.Heatmap(\n",
        "        z=similarity_matrix,\n",
        "        x=[f'C{i}' for i in range(optimal_n_clusters)],  # Labels mais curtos\n",
        "        y=[f'C{i}' for i in range(optimal_n_clusters)],\n",
        "        colorscale='RdYlBu_r',\n",
        "        zmin=0,\n",
        "        zmax=1,\n",
        "        text=text_matrix,\n",
        "        texttemplate=\"%{text}\",\n",
        "        textfont={\"size\": 12, \"color\": \"black\", \"family\": \"Arial Bold\"},\n",
        "        hovertemplate='<b>Similaridade entre %{y} e %{x}</b><br>Valor: %{z:.3f}<extra></extra>',\n",
        "        showscale=True,\n",
        "        colorbar=dict(\n",
        "            title=dict(\n",
        "                text=\"Similaridade\",\n",
        "                side=\"right\",\n",
        "                font=dict(size=16)\n",
        "            ),\n",
        "            tickmode=\"linear\",\n",
        "            tick0=0,\n",
        "            dtick=0.2,\n",
        "            len=0.8,\n",
        "            thickness=25,\n",
        "            tickfont=dict(size=14)\n",
        "        ),\n",
        "        xgap=2,  # Espa√ßamento entre c√©lulas\n",
        "        ygap=2\n",
        "    ))\n",
        "\n",
        "    fig3.update_layout(\n",
        "        title=dict(\n",
        "            text=\"Gr√°fico 3: Matriz de Similaridade Entre Clusters\",\n",
        "            font=dict(size=18, family=\"Arial Black\")\n",
        "        ),\n",
        "        xaxis_title=\"Clusters\",\n",
        "        yaxis_title=\"Clusters\",\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        template=\"plotly_white\",\n",
        "        margin=dict(l=80, r=80, t=100, b=80),\n",
        "        xaxis=dict(\n",
        "            tickfont=dict(size=14),\n",
        "            title_font=dict(size=16),\n",
        "            side=\"bottom\",\n",
        "            tickangle=0\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickfont=dict(size=14),\n",
        "            title_font=dict(size=16),\n",
        "            autorange=\"reversed\",\n",
        "            tickangle=0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig3.show()\n",
        "\n",
        "    return {\n",
        "        'cluster_stats': cluster_stats,\n",
        "        'similarity_matrix': similarity_matrix,\n",
        "        'cluster_centroids': cluster_centroids\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCFXd3lZsSFV"
      },
      "outputs": [],
      "source": [
        "# Cria visualiza√ß√£o de confian√ßa inspirada no c√≥digo do professor\n",
        "# Mostra acertos vs erros com tamanho proporcional √† confian√ßa da predi√ß√£o\n",
        "def create_confidence_analysis_plot(questions, embeddings, reductions, cluster_results, confidence_data):\n",
        "\n",
        "    y_true = cluster_results[\"kmeans\"]  # Clusters verdadeiros\n",
        "    y_pred = confidence_data['predictions']  # Predi√ß√µes baseadas em similaridade\n",
        "    y_conf = confidence_data['confidence']  # Confian√ßa\n",
        "\n",
        "    # Calcular acertos (se predi√ß√£o = cluster verdadeiro)\n",
        "    is_correct = (y_true == y_pred).astype(int)\n",
        "\n",
        "    # Criar tamanhos baseados na confian√ßa\n",
        "    sizes = 20 + 80 * y_conf\n",
        "\n",
        "    fig4 = go.Figure()\n",
        "\n",
        "    # Pontos corretos (verde)\n",
        "    correct_mask = is_correct == 1\n",
        "    fig4.add_trace(\n",
        "            go.Scatter(\n",
        "            x=reductions[\"tsne_2d\"][correct_mask, 0],\n",
        "            y=reductions[\"tsne_2d\"][correct_mask, 1],\n",
        "                mode='markers',\n",
        "            marker=dict(\n",
        "                size=sizes[correct_mask],\n",
        "                color='green',\n",
        "                opacity=0.7,\n",
        "                line=dict(width=1, color='darkgreen')\n",
        "            ),\n",
        "            name='Predi√ß√µes Corretas',\n",
        "            hovertemplate='<b>Acerto</b><br>Confian√ßa: %{marker.size}<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Pontos incorretos (vermelho) - apenas se houver erros\n",
        "    incorrect_mask = is_correct == 0\n",
        "    if incorrect_mask.any():\n",
        "        fig4.add_trace(\n",
        "            go.Scatter(\n",
        "                x=reductions[\"tsne_2d\"][incorrect_mask, 0],\n",
        "                y=reductions[\"tsne_2d\"][incorrect_mask, 1],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=sizes[incorrect_mask],\n",
        "                    color='red',\n",
        "                    opacity=0.7,\n",
        "                    line=dict(width=1, color='darkred')\n",
        "                ),\n",
        "                name='Predi√ß√µes Incorretas',\n",
        "                hovertemplate='<b>Erro</b><br>Confian√ßa: %{marker.size}<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # Se n√£o h√° erros, adicionar uma anota√ß√£o informativa\n",
        "        fig4.add_annotation(\n",
        "            x=0.5,\n",
        "            y=0.95,\n",
        "            xref=\"paper\",\n",
        "            yref=\"paper\",\n",
        "            showarrow=False,\n",
        "            font=dict(size=16, color='green'),\n",
        "            bgcolor='rgba(255,255,255,0.8)',\n",
        "            bordercolor='green',\n",
        "            borderwidth=2\n",
        "        )\n",
        "\n",
        "    fig4.update_layout(\n",
        "        title=dict(\n",
        "            text=\"Gr√°fico 4: An√°lise de Confian√ßa e Qualidade\",\n",
        "            font=dict(size=18, family=\"Arial Black\")\n",
        "        ),\n",
        "        xaxis_title=\"t-SNE Dimens√£o 1\",\n",
        "        yaxis_title=\"t-SNE Dimens√£o 2\",\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        showlegend=True,\n",
        "        template=\"plotly_white\",\n",
        "        margin=dict(l=80, r=80, t=100, b=80),\n",
        "        xaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            showgrid=True,\n",
        "            gridwidth=1,\n",
        "            gridcolor='lightgray',\n",
        "            title_font=dict(size=14),\n",
        "            tickfont=dict(size=12)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig4.show()\n",
        "\n",
        "    # An√°lise de erros muito confiantes\n",
        "    high_conf_threshold = np.nanpercentile(y_conf, 85)\n",
        "    high_conf_errors = (is_correct == 0) & (y_conf >= high_conf_threshold)\n",
        "\n",
        "    print(f\"\\nüìä AN√ÅLISE DE QUALIDADE:\")\n",
        "    print(f\"   ‚Ä¢ Taxa de acerto geral: {(is_correct.mean() * 100):.1f}%\")\n",
        "    print(f\"   ‚Ä¢ Confian√ßa m√©dia: {y_conf.mean():.3f}\")\n",
        "    print(f\"   ‚Ä¢ Confian√ßa m√°xima: {y_conf.max():.3f}\")\n",
        "    print(f\"   ‚Ä¢ Confian√ßa m√≠nima: {y_conf.min():.3f}\")\n",
        "\n",
        "    if incorrect_mask.any():\n",
        "        print(f\"   ‚Ä¢ Erros muito confiantes (‚â•{high_conf_threshold:.2f}): {high_conf_errors.sum()}\")\n",
        "        max_error_conf = y_conf[incorrect_mask].max()\n",
        "        print(f\"   ‚Ä¢ Maior confian√ßa em erro: {max_error_conf:.3f}\")\n",
        "    else:\n",
        "        print(f\"   ‚Ä¢ Todas as predi√ß√µes est√£o corretas com confian√ßa m√©dia de {y_conf.mean():.3f}\")\n",
        "        max_error_conf = 0.0\n",
        "\n",
        "    return {\n",
        "        'accuracy': is_correct.mean(),\n",
        "        'avg_confidence': y_conf.mean(),\n",
        "        'high_confidence_errors': high_conf_errors.sum(),\n",
        "        'max_error_confidence': max_error_conf\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhA3-RSDsTvg"
      },
      "outputs": [],
      "source": [
        "# Analisa confian√ßa e qualidade do modelo baseado na similaridade com centroides\n",
        "# Simula predi√ß√µes e calcula m√©tricas de confian√ßa para identificar erros\n",
        "def analyze_model_confidence_and_quality(questions, embeddings, cluster_results, model, tokenizer):\n",
        "\n",
        "\n",
        "    # Simular predi√ß√µes baseadas na similaridade com centroides\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    cluster_labels = cluster_results[\"kmeans\"]\n",
        "    cluster_centroids = []\n",
        "\n",
        "    for cluster_id in range(len(np.unique(cluster_labels))):\n",
        "        mask = cluster_labels == cluster_id\n",
        "        if np.any(mask):\n",
        "            centroid = np.mean(embeddings[mask], axis=0)\n",
        "            cluster_centroids.append(centroid)\n",
        "        else:\n",
        "            cluster_centroids.append(np.zeros(embeddings.shape[1]))\n",
        "\n",
        "    cluster_centroids = np.array(cluster_centroids)\n",
        "\n",
        "    # Calcular similaridades com todos os centroides\n",
        "    similarities = cosine_similarity(embeddings, cluster_centroids)\n",
        "\n",
        "    # Predi√ß√µes baseadas no centroide mais pr√≥ximo\n",
        "    y_pred = np.argmax(similarities, axis=1)\n",
        "\n",
        "    # Confian√ßa baseada na diferen√ßa entre o melhor e segundo melhor\n",
        "    sorted_similarities = np.sort(similarities, axis=1)\n",
        "    confidence = sorted_similarities[:, -1] - sorted_similarities[:, -2]\n",
        "\n",
        "    # Normalizar confian√ßa para 0-1\n",
        "    confidence = (confidence - confidence.min()) / (confidence.max() - confidence.min() + 1e-8)\n",
        "\n",
        "    return {\n",
        "        'predictions': y_pred,\n",
        "        'confidence': confidence,\n",
        "        'similarities': similarities,\n",
        "        'cluster_centroids': cluster_centroids\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9uUXDQksVKv"
      },
      "outputs": [],
      "source": [
        "# Identifica varia√ß√µes lingu√≠sticas dentro dos clusters\n",
        "# Encontra perguntas com significados similares mas formuladas de forma diferente\n",
        "def analyze_linguistic_variations(questions, embeddings, cluster_labels, threshold=0.8):\n",
        "\n",
        "\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Calcular similaridade coseno\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    variations = {}\n",
        "\n",
        "    for cluster_id in np.unique(cluster_labels):\n",
        "        if cluster_id == -1:  # Pular outliers do DBSCAN\n",
        "            continue\n",
        "\n",
        "        cluster_mask = cluster_labels == cluster_id\n",
        "        cluster_questions = [questions[i] for i in range(len(questions)) if cluster_mask[i]]\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "\n",
        "        if len(cluster_indices) < 2:\n",
        "            continue\n",
        "\n",
        "        # Encontrar pares similares dentro do cluster\n",
        "        cluster_similarities = similarity_matrix[np.ix_(cluster_indices, cluster_indices)]\n",
        "\n",
        "        similar_pairs = []\n",
        "        for i in range(len(cluster_indices)):\n",
        "            for j in range(i+1, len(cluster_indices)):\n",
        "                # Evitar compara√ß√£o da mesma pergunta consigo mesma\n",
        "                if i != j and cluster_similarities[i, j] >= threshold:\n",
        "                    similar_pairs.append({\n",
        "                        \"question1\": cluster_questions[i],\n",
        "                        \"question2\": cluster_questions[j],\n",
        "                        \"similarity\": cluster_similarities[i, j]\n",
        "                    })\n",
        "\n",
        "        if similar_pairs:\n",
        "            variations[f\"cluster_{cluster_id}\"] = {\n",
        "                \"total_questions\": len(cluster_questions),\n",
        "                \"similar_pairs\": similar_pairs,\n",
        "                \"variation_rate\": len(similar_pairs) / (len(cluster_questions) * (len(cluster_questions) - 1) / 2)\n",
        "            }\n",
        "\n",
        "    return variations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pppm2oGCsWli"
      },
      "outputs": [],
      "source": [
        "# Encontra exemplos representativos de cada cluster\n",
        "# Seleciona as perguntas mais pr√≥ximas ao centroide de cada grupo\n",
        "def analyze_cluster_examples(questions, cluster_labels, embeddings, n_examples=5):\n",
        "\n",
        "\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    cluster_examples = {}\n",
        "\n",
        "    for cluster_id in np.unique(cluster_labels):\n",
        "        if cluster_id == -1:  # Pular outliers do DBSCAN\n",
        "            continue\n",
        "\n",
        "        cluster_mask = cluster_labels == cluster_id\n",
        "        cluster_questions = [questions[i] for i in range(len(questions)) if cluster_mask[i]]\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "        cluster_embeddings = embeddings[cluster_indices]\n",
        "\n",
        "        if len(cluster_questions) < 2:\n",
        "            continue\n",
        "\n",
        "        # Encontrar o centroide do cluster\n",
        "        centroid = np.mean(cluster_embeddings, axis=0)\n",
        "\n",
        "        # Calcular similaridade com o centroide\n",
        "        similarities = cosine_similarity([centroid], cluster_embeddings)[0]\n",
        "\n",
        "        # Pegar os exemplos mais pr√≥ximos ao centroide\n",
        "        top_indices = np.argsort(similarities)[-n_examples:][::-1]\n",
        "\n",
        "        cluster_examples[f\"cluster_{cluster_id}\"] = {\n",
        "            \"total_questions\": len(cluster_questions),\n",
        "            \"representative_examples\": [\n",
        "                {\n",
        "                    \"question\": cluster_questions[idx],\n",
        "                    \"similarity_to_centroid\": similarities[idx]\n",
        "                }\n",
        "                for idx in top_indices\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    return cluster_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBXAN5kwsYB_"
      },
      "outputs": [],
      "source": [
        "# Preparar dados\n",
        "questions = df_trainable[\"instruction\"].tolist()\n",
        "if CLUSTERING_CONFIG[\"max_samples\"] and len(questions) > CLUSTERING_CONFIG[\"max_samples\"]:\n",
        "    # Amostragem aleat√≥ria para datasets grandes\n",
        "    np.random.seed(CLUSTERING_CONFIG[\"random_state\"])\n",
        "    indices = np.random.choice(len(questions), CLUSTERING_CONFIG[\"max_samples\"], replace=False)\n",
        "    questions = [questions[i] for i in indices]\n",
        "    print(f\"Usando amostra de {len(questions)} perguntas de {len(df_trainable)} total\")\n",
        "\n",
        "print(f\"Extraindo embeddings de {len(questions)} perguntas...\")\n",
        "\n",
        "# Extrair embeddings usando o modelo fine-tunado\n",
        "embeddings = extract_question_embeddings(\n",
        "    merged, tok, questions,\n",
        "    strategy=CLUSTERING_CONFIG[\"embedding_strategy\"]\n",
        ")\n",
        "\n",
        "print(f\"Embeddings extra√≠dos: {embeddings.shape}\")\n",
        "\n",
        "# Reduzir dimens√µes\n",
        "reductions = reduce_dimensions(embeddings, CLUSTERING_CONFIG)\n",
        "\n",
        "# Otimizar n√∫mero de clusters\n",
        "optimization_results = find_optimal_clusters(embeddings, CLUSTERING_CONFIG)\n",
        "\n",
        "print(f\"N√∫mero √≥timo de clusters (Silhouette): {optimization_results['optimal_silhouette']}\")\n",
        "print(f\"N√∫mero √≥timo de clusters (Calinski-Harabasz): {optimization_results['optimal_calinski']}\")\n",
        "\n",
        "# Usar o n√∫mero √≥timo baseado no Silhouette Score\n",
        "optimal_n_clusters = optimization_results['optimal_silhouette']\n",
        "\n",
        "# Aplicar algoritmos de clustering\n",
        "cluster_results = apply_clustering_algorithms(embeddings, optimal_n_clusters, CLUSTERING_CONFIG)\n",
        "\n",
        "# Criar visualiza√ß√µes principais\n",
        "visualization_data = create_main_visualizations(questions, embeddings, reductions, cluster_results, optimal_n_clusters)\n",
        "\n",
        "# An√°lise adicional\n",
        "confidence_data = analyze_model_confidence_and_quality(questions, embeddings, cluster_results, merged, tok)\n",
        "quality_metrics = create_confidence_analysis_plot(questions, embeddings, reductions, cluster_results, confidence_data)\n",
        "\n",
        "# Analisar varia√ß√µes lingu√≠sticas\n",
        "linguistic_variations = analyze_linguistic_variations(\n",
        "    questions, embeddings, cluster_results[\"kmeans\"], threshold=0.7\n",
        ")\n",
        "\n",
        "# Analisar exemplos representativos dos clusters\n",
        "cluster_examples = analyze_cluster_examples(\n",
        "    questions, cluster_results[\"kmeans\"], embeddings, n_examples=3\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"\\nüìä ESTAT√çSTICAS GERAIS:\")\n",
        "print(f\"   ‚Ä¢ Total de perguntas analisadas: {len(questions)}\")\n",
        "print(f\"   ‚Ä¢ Dimens√£o dos embeddings: {embeddings.shape[1]}\")\n",
        "print(f\"   ‚Ä¢ N√∫mero √≥timo de clusters: {optimal_n_clusters}\")\n",
        "\n",
        "print(f\"   ‚Ä¢ Melhor Silhouette Score: {max(optimization_results['silhouette_scores']):.3f}\")\n",
        "print(f\"   ‚Ä¢ Melhor Calinski-Harabasz Score: {max(optimization_results['calinski_scores']):.3f}\")\n",
        "\n",
        "print(f\"\\nüîç DISTRIBUI√á√ÉO DOS CLUSTERS (K-Means):\")\n",
        "cluster_counts = np.bincount(cluster_results[\"kmeans\"])\n",
        "for i, count in enumerate(cluster_counts):\n",
        "    percentage = (count / len(questions)) * 100\n",
        "    print(f\"   ‚Ä¢ Cluster {i}: {count} perguntas ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìà QUALIDADE DOS CLUSTERS:\")\n",
        "cluster_stats = visualization_data['cluster_stats']\n",
        "avg_similarity = cluster_stats['avg_similarity'].mean()\n",
        "avg_density = cluster_stats['density_score'].mean()\n",
        "print(f\"   ‚Ä¢ Similaridade m√©dia interna: {avg_similarity:.3f}\")\n",
        "print(f\"   ‚Ä¢ Densidade m√©dia: {avg_density:.3f}\")\n",
        "\n",
        "print(f\"\\nüéØ M√âTRICAS DE QUALIDADE:\")\n",
        "print(f\"   ‚Ä¢ Taxa de acerto: {quality_metrics['accuracy']:.1%}\")\n",
        "print(f\"   ‚Ä¢ Confian√ßa m√©dia: {quality_metrics['avg_confidence']:.3f}\")\n",
        "print(f\"   ‚Ä¢ Erros muito confiantes: {quality_metrics['high_confidence_errors']}\")\n",
        "print(f\"   ‚Ä¢ Maior confian√ßa em erro: {quality_metrics['max_error_confidence']:.3f}\")\n",
        "\n",
        "print(f\"\\nüîç VARIA√á√ïES LINGU√çSTICAS IDENTIFICADAS:\")\n",
        "total_variations = 0\n",
        "for cluster_name, variation_data in linguistic_variations.items():\n",
        "    cluster_id = cluster_name.split(\"_\")[1]\n",
        "    similar_pairs = len(variation_data[\"similar_pairs\"])\n",
        "    total_variations += similar_pairs\n",
        "    if similar_pairs > 0:\n",
        "        print(f\"   ‚Ä¢ Cluster {cluster_id}: {similar_pairs} pares similares\")\n",
        "\n",
        "print(f\"\\n‚úÖ RESUMO FINAL:\")\n",
        "print(f\"   ‚Ä¢ Sistema identifica {total_variations} varia√ß√µes lingu√≠sticas\")\n",
        "print(f\"   ‚Ä¢ Taxa de varia√ß√£o m√©dia: {(total_variations / len(questions)) * 100:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Clusters bem definidos com Silhouette Score de {max(optimization_results['silhouette_scores']):.3f}\")\n",
        "print(f\"   ‚Ä¢ Taxa de acerto: {quality_metrics['accuracy']:.1%} com confian√ßa m√©dia de {quality_metrics['avg_confidence']:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
